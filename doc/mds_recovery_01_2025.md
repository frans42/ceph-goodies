# Postmortem: Two weeks loss of access to a ceph file system

A ceph cluster version Pacific or newer might be affected if the following conditions are met simultaneously

- An MDS rank reports more stray items than can fit in an OPS write queue (varies, in our case about 37Mio strays were the limit and the rank reported 100Mio stray items).
- This rank also reports an over-sized MDS cache (in our case about 13 times the configured memory limit).

Under these conditions, a restart of the rank can lead to the rank becoming and staying unresponsive after startup. With a single MDS the file system will be inaccessible immediately. In a multi-MDS set up, access to the file system is lost gradually until a total outage of access occurs. Restarting the affected rank can unblock access for a while, but has no lasting effect.

**Solution:** Set the MDS memory limit for the critical rank to a value greater than `(cache size reported in warning)/(1-(mds cache reservation))` and fail the MDS onto a host with sufficient RAM+swap. During startup follow the number of stray items until it stabilizes. Give the MDS time to trim caches until the cache size doesn't change any more. Slowly reduce the memory limit to normal and let the MDS trim caches gradually.

This was the two-week long story short. For the interested reader we provide the long story below with the intent of giving some guidelines for how to rescue a ceph cluster from a major incident like this.


## What happened

On Friday, January 20, 2025 we failed a rank to initiate cache trimming and stray evaluation. This rank became unresponsive after startup and access to the file system was gradually lost. What followed were 13 nerve-wrecking days of investigation and recovery attempts.


## How did we get there

Well, that story started a long time ago, dating back all the way to our mimic times.

Until ceph version Pacific there was a long-standing limitation on the number of files and directories that can be deleted when snapshots are present. Simply speaking, when files and directories are deleted while snapshots are present, the data is not actually deleted, but moved to internal directories referred to as *stray buckets*. Every MDS rank ownes 10 stray buckets and, until Octopus, these buckets were not fragmented like ordinary directories.

Consequently, if a delete operation was attempted that would lead to the stray buckets exceeding `mds_bal_fragment_size_max` entries (*stray items*), the user would see a "no space left on device" error message and the delete was not executed. The counter-intuitiveness of this error message added a lot of wrinkles to our users' foreheads and amusing tickets to our case system. Unfortunately, it also implied an operational limitation of the ceph file system that we needed to work around.

The default value for `mds_bal_fragment_size_max` is 100000, providing a total of 1Mio top-level stray items. Since each stray item in itself can be a directory with a sub-tree structure, this is often enough. Unless one operates an HPC cluster where users run applications such as OpenFOAM that create millions of files and directories in a single simulation run and a lot of these results are temporary. Its not only OpenFOAM, we have also seen Comsol and hand-written python code going bonkers. OpenFOAM is most notorious though as this is standard behavior while for other applications its a bug.

To deal with these "no space left on device" situations, our usual approach was to increase `mds_bal_fragment_size_max` temporarily and wait for a snapshot rotation to complete, after which the stray count would usually return to manageable numbers. Until it didn't. We discovered the hard way that there is a situation where strays don't disappear after a snapshot rotation, see [num_stray growing without bounds](https://www.spinics.net/lists/ceph-users/msg73150.html). This is where hard-links come into play. A hard link in a snapshot somewhere can prevent purging of strays everywhere and we had boatloads of hard-links coming from anaconda installations that got updated regularly. The only way to remove such orphaned strays (a stray that is no longer present on the file system or in any snapshot) is to restart the MDS. Orphaned strays are "evaluated" (purged) as part of MDS startup.

This procedure worked reasonably well until we had one of these cases that was also different. As usual, I informed the user in question to stop the delete process and limit future delete operations to about half a million files per week. To which the user replied something like "I have this folder with 50Mio files and directories I need to delete. Could you please explain to me how to do that?" Obviously not wanting to do small deletes every week for the next two years, not to mention further temporary files that will pile up on top of that. By that time we were on Octopus and already knew that Pacific would resolve this limitation. However, we urgently needed some bug fixes that were only going to show up in the last bug-fix release of Pacific. This final release was delayed until summer 2024 - something I was actually happy about, because it meant some important extra fixes Pacific would not have seen otherwise.

Back to the story. Our workaround was no longer an acceptable solution, we needed something else. Knowing that an upgrade to Pacific was around the corner, we decided to provide users with a way to move large trees out of their working directories and we would clean up this trash once the upgrade was done. We implemented a "trash" command with an environment module called "storage/ceph" on our HPC cluster, which would execute a very verbose move to `.trash` folders we created somewhere else. As a consequence of "moving files to a trash location" instead of "deleting files permanently" we accumulated about 600Mio trash files and directories over a period of one and a half years.

The upgrade to Pacific finally happened on December 10th 2024. Since a clean-up of 600Mio files and folders causes very high load on the storage servers, we decided to execute the delete operation over the Christmas holidays. For about 2 weeks all storage servers were continuously at 100-150% load and at the beginning of January the files were finally deleted from the file system. The last step to do was to wait for these deleted files to be rotated out of the snapshot retention and clean up the strays from the MDS daemons. As explained above, until Octopus the usual procedure was to fail a rank that would then purge strays as part of the initialization of the newly assigned MDS. With Pacific a new way for purging strays on-line (no MDS restart required), a so-called "forward scrub" of the file system was introduced and that is what we initiated.

Unfortunately, what the documentation forgot to mention was that a forward scrub requires a huge amount of RAM, much more than we have installed. We had to stop the forward scrub to prevent the MDS daemons from being OOM killed. We waited for some time to see if the memory allocated during the scrub would start going down again without intervention, but unfortunately it didn't. At this point we decided to go old school and restart the MDS daemons to re-initiate cache trimming (reduce RAM usage) and also have the strays purged on start up. This worked well for 7 out of 8 ranks. For the sake of bad luck, the vast majority of stray items accumulated on a single rank, rank 2, which was holding 100Mio stray items. The other ranks had no more than 2Mio and restarted without any issues. With rank 2, however, we realized very quickly we had a problem.

The first indication of trouble was that the MDS on that rank started to consume significantly more RAM than it did before the restart. I anxiously watched as less and less out of 512G installed RAM was available, hoping the increase of consumption would stop at some point - but it didn't. There were lot of ceph-user cases with similar problems in the past and the usual solution was to add a lot of swap. Fortunately for us, the host had a free disk slot and we had a good amount of spare hardware available. As a first measure we stopped all OSDs running on this host, which freed up all of the host's 512G for this rank. While observing the continuing growth of consumption we prepared a fast 4TB SSD to be installed as a swap device.

Somewhere around 400G resident size the MONs failed rank 2 and assigned a stand-by MDS to that rank. We had another problem: the MDS became unresponsive. That was a really bad sign. To prevent a round-robin MDS-fail shootout we increased `mds_beacon_grace` to something like 12 hours and failed the rank back to the host with all OSDs down. At this point it was kind of obvious that the installed RAM is likely not enough and we added the swap disk. We managed to do this in time before the MDS would go OOM. Back in the office we waited again to see how much RAM the MDS would need and what would happen after its start-up. Good news was that it needed only about 700G RAM. Bad news was that it stayed unresponsive, which means that the entire file system would become unresponsive within a short period of time as every other rank will eventually be stuck with a blocking operation requiring rank 2 to respond. At this point we submitted a request for help to the ceph-user list.

Now that we had sufficient RAM on that host we decided to bring the OSDs up again. To avoid excessive swap-in and -out we restarted everything, including MDS rank 2. This brought all OSDs back up with recovery finished well before the host would start swapping due to the large MDS. Everything came back up as before and the MDS would eventually get stuck. A strange first observation was that the MDS was idle while being stuck, it was not producing any CPU load any more. The MDS was marked active and there were no health errors indicating a limitation of data access or a problem with data integrity. Nothing obvious or out of the ordinary, the file system should just work. Instead, blocked MDS-OPS warnings started showing up and file system activity began to drop.

At this point it was really late and the most simple next step to try was to wait for a while to see if something happens by itself. Conveniently when it was time to sleep. Can't say I slept well though.

### To be continued
